{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning を用いた Image Captioning\n",
    "\n",
    "## 今日のゴール\n",
    "* Deep Learningを用いたImage Captioningの概要を理解する\n",
    "* (基本的な手法をコードで書けるようになる)\n",
    "* 近年の研究動向や手法以外の重要な構成要素について知る\n",
    "\n",
    "## 前提知識\n",
    "Neural Network (NN) の概要  \n",
    "　　Neural Networkがどのようなものなのかをなんとなく把握している\n",
    "  \n",
    "## 知っとくと話が早い知識\n",
    "Convolutional Neural Network (CNN) の概要  \n",
    "Recurrent Neural Network (RNN) の概要  \n",
    "Embeddingと呼ばれる手法の概要  \n",
    "\n",
    "## 今日の方針\n",
    "細かなところは突っ込まず、概要を把握することを重視する  \n",
    "  - そのためかなりゆるふわ説明でいきます\n",
    "  - 細かなところを知りたい人は別途聞いてください\n",
    "  \n",
    "よろしくお願いします！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 本日のアジェンダ\n",
    "* そもそもImage Captioningって？\n",
    "* Deep Learning を用いた Image Captioning を理解するための5ステップ\n",
    "* 実装のノウハウ\n",
    "* データセットのお話\n",
    "* 評価方法のお話\n",
    "* Image Captioningの向かう先・近年の研究動向"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# そもそもImage Captioningって？\n",
    "![Image Captioningの例](http://mscoco.cloudapp.net/static/images/captions-challenge2015.jpg)\n",
    "<div style=\"text-align: center;\">\n",
    "Image Captioningのイメージ\n",
    "引用：MSCOCO\n",
    "</div>\n",
    "画像の内容を説明するような自然文を、画像のみから自動で生成する手法のこと。\n",
    "\n",
    "## できると何が嬉しい？\n",
    "色々嬉しい！！\n",
    "* 学術面では  \n",
    "　　- 画像はコンピュータの目。画像処理の究極の目標は「画像の\"内容\"理解」。ここを目指す研究。  \n",
    "　　- 言語に直せるということは（言語処理の部分がうまく作動するかはおいといて）理解しているということでは？  \n",
    "　　- 結果の解釈も容易  \n",
    "* 応用分野は  \n",
    "　　- 自動運転、スマートカーナビへの適用  \n",
    "　　- 視覚障害者向けのSNSの画像読み上げ  \n",
    "　　- 詳細な画像アノテーション、画像検索への応用  \n",
    "  ...\n",
    "  \n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning を用いた Image Captioning を理解するための5ステップ\n",
    "\n",
    "## ステップ1 CNN\n",
    "\n",
    "![CNNの概要図](https://www.researchgate.net/profile/Siva_Chaitanya_Mynepalli/publication/281607765/figure/fig1/AS:284643598323714@1444875730488/Figure-1-Learning-hierarchy-of-visual-features-in-CNN-architecture.png)\n",
    "<div style=\"text-align: center;\">\n",
    "CNNの概要図  \n",
    "引用：https://www.researchgate.net/figure/281607765_fig1_Figure-1-Learning-hierarchy-of-visual-features-in-CNN-architecture\n",
    "</div>\n",
    "\n",
    "* CNNは画像処理の分野を中心に様々な分野で大きな成果を上げているNNのアーキテクチャ\n",
    "* 特に画像認識の分野では人間を超えた性能を発揮している\n",
    "* CNNの訓練では、(一般的に)入力となる生データとそれに付与する正解ラベルのペアをのみを与える\n",
    "* CNNは訓練データから人間が定めた目的関数を最小化するようにパラメータを調整する(よい特徴を捉えられるようにパラメータを調整する)\n",
    "* あるタスクで訓練されたCNNのパラメータはそのタスクに類似したり包含関係にあるタスクを解くために利用できることが知られている（fine tuning）\n",
    "\n",
    "\n",
    "## ステップ2 RNNを用いた言語モデル\n",
    "\n",
    "![RNNの概要図](https://image.slidesharecdn.com/20150831jcsssummer-150901075627-lva1-app6892/95/-44-638.jpg?cb=1441094377)\n",
    "<div style=\"text-align: center;\">\n",
    "RNNを用いた言語モデルの概要図  \n",
    "引用：言語と知識の深層学習@認知科学会サマースクール\n",
    "</div>\n",
    "\n",
    "* RNNは言語処理の分野を中心に大きな成果を上げているNNのアーキテクチャ\n",
    "* 言語モデルとは人間が扱う「言葉」らしさを確率で扱うためのモデルのこと\n",
    "* 言語モデルの分野では、RNNを用いた手法がデファクトスタンダードとなっている\n",
    "* RNNを用いた言語モデルでは、ある単語を入力として、その次に出てくる単語の確率を推定する。これを繰り返して、\"言語らしい\"文章を生成する。\n",
    "\n",
    "\n",
    "## ステップ3 Embedding\n",
    "![cross domain embeddingの例](https://qph.ec.quoracdn.net/main-qimg-e8b83b14d7261d75754a92d0d3605e36)\n",
    "<div style=\"text-align: center;\">\n",
    "Embedding後の空間の例  \n",
    "引用：[What is word embedding in deep learning?](https://www.quora.com/What-is-word-embedding-in-deep-learning)\n",
    "</div>\n",
    "\n",
    "* Embeddingは画像や文書、単語と行った構造的なデータを低次元のベクトル空間に\"埋め込む\"手法のこと。word2vecなどが有名。(数学的には、数学的構造間の構造を保つような単射のことをEmbeddingという)\n",
    "* CNNでもRNNでも、タスクを解くためにアーキテクチャの中で生データが低次元のベクトルに変換されている。これは擬似的にEmbeddingをしているとも捉えることができる。（実際は、変換後のベクトルに構造的な特徴があるかは不明なためEmbeddingとは言えない気がする）\n",
    "* NNのアーキテクチャや目的関数を工夫することで、画像と単語のような異なるドメインのデータを同じベクトル空間にEmbeddingできることが知られている。\n",
    "\n",
    "![画像-単語のembeddingの例](https://ai2-s2-public.s3.amazonaws.com/figures/2017-08-08/2e36ea91a3c8fbff92be2989325531b4002e2afc/12-Figure4-1.png)\n",
    "<div style=\"text-align: center;\">\n",
    "画像-単語のEmbeddingの例    \n",
    "引用：[Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models](https://www.semanticscholar.org/paper/Unifying-Visual-Semantic-Embeddings-with-Multimoda-Kiros-Salakhutdinov/2e36ea91a3c8fbff92be2989325531b4002e2afc)\n",
    "</div>\n",
    "\n",
    "* NNでは、**画像だろうと自然言語だろうとベクトルに変換してしまえば同じように扱う**ことができる\n",
    "\n",
    "## ステップ4 seq2seqを用いた機械翻訳\n",
    "\n",
    "![seq2seqの概要図](https://camo.qiitausercontent.com/2efbd678be5ba58363152092695e959571dbf94c/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3130303639382f39343962313933302d356462322d303038632d346462312d6164323462326565613466652e706e67)\n",
    "<div style=\"text-align: center;\">\n",
    "seq2seqを用いた機械翻訳の概要図  \n",
    "引用：[ChainerとRNNと機械翻訳](https://qiita.com/odashi_t/items/a1be7c4964fbea6a116e)\n",
    "</div>\n",
    "\n",
    "* seq2seqは機械翻訳や音声生成などの分野で大きな成果を上げているNNのアーキテクチャ\n",
    "* seq2seqはencoderとdecoderのコンポーネントに分けられる\n",
    "* seq2seqを用いた機械翻訳では、encoderはソースとなる言語の文章をRNNを用いた言語モデルへ入力し、その文章を表現するような低次元のベクトルを生成する。decoderではencoderが生成したベクトルを入力として、ターゲットとなる言語の文章を生成していく。\n",
    "* seq2seqの長所は、encoderとdecoderの設計を工夫することで、様々なタスクへ応用できること。\n",
    "\n",
    "## ステップ5 CNNとRNNによるseq2seqを用いたImage Captioning\n",
    "![image captioningのNNの具体例](https://gigaom.com/wp-content/uploads/sites/1/2014/11/googlernncnn.png)\n",
    "<div style=\"text-align: center;\">\n",
    "CNNとRNNによるseq2seqを用いたImage Captioningの概要図  \n",
    "引用：[Show and Tell: A Neural Image Caption Generator](https://arxiv.org/abs/1411.4555)\n",
    "</div>\n",
    "\n",
    "* Image Captioningのタスクを**画像から文章への翻訳**のタスクと考える。そうすると、seq2seqモデルを適用して解くことができる。\n",
    "* encoderではCNNを用いて画像を表現する低次元のベクトルを生成する。decoderではCNNの出力を入力として、RNNによる言語モデルを用いて文章を生成する。\n",
    "* 訓練では、入力とする生データである画像と、そのデータの正解ラベルである画像に対する文章のペアを与えて、以下の目的関数を最大化するようにEnd-to-Endでパラメータの調整を行う。\n",
    "\n",
    "\\begin{equation}\n",
    "  \\theta^* = argmax_{\\theta} \\sum_{(I, S)} (S  |  I; \\theta)\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 実装のノウハウ\n",
    "ここまででDeep Learningを用いた最も基本的なImage Captioningシステムの概要は把握できたはずなので、実際に実装を行うことを考えてみる。\n",
    "実際の実装の手順は以下の通り。\n",
    "\n",
    "1. encoderとなるCNNの構造を定義する。\n",
    "2. 1.で定義したCNNを画像認識のタスクで事前学習を行う。これを行わないと、学習に時間がかかってしまったり、うまく学習できなかったりする。\n",
    "3. decoderとなるRNNの構造を定義し、CNNとつなげてseq2seqモデルを構築する\n",
    "4. 以下に示す目的関数を最小化するようにパラメータの調整を行う\n",
    "\n",
    "\\begin{equation}\n",
    "  L(I, S) = -\\sum_{n=1}^{N}\\log p_t(S_t)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import chainer\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "from chainer import initializers\n",
    "\n",
    "class CNNEncoder(chainer.Chain):\n",
    "\n",
    "    def __init__(self):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def __call__(self, x):\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "class RNNDecoder(chainer.Chain):\n",
    "\n",
    "    def __init__(self):\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "class Seq2SeqImageCaptionModel(chainer.Chain):\n",
    "    def __init__(self, n_units, n_out):\n",
    "        super(Seq2SeqImageCaptionModel, self).__init__()\n",
    "        with self.init_scope():\n",
    "            self.enc = CNNEncoder()\n",
    "            self.dec = RNNDecoder()\n",
    "            \n",
    "    def init_state(self, img_feature):\n",
    "        \"\"\"\n",
    "        encoderの出力でRNNの重みを初期化\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def __call__(self, x):\n",
    "        h = CNNEncoder(x)\n",
    "        y = RNNDecoder(h)\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データセットのお話\n",
    "Image Captioningが登場し発展したのは、このモデルを訓練するための高品質なデータセットが登場したため。  \n",
    "以下に代表的なデータセットを示す  \n",
    "\n",
    "## flicker8k, 30k\n",
    "http://nlp.cs.illinois.edu/HockenmaierGroup/8k-pictures.html # 8k  \n",
    "Image Captioningのデータセットの先駆け的な存在。  \n",
    "flickerに投稿されたCCライセンスの画像に人間が説明文をアノテーションしたもの  \n",
    "\n",
    "## MSCOCO\n",
    "http://cocodataset.org/#home  \n",
    "最も広く使われているデータセット  \n",
    "約80k枚（だった気がする）の画像ごとに5つの説明文が付与されている  \n",
    "Yahoo! Japanがこれを日本語に翻訳したデータセットを公開している\n",
    "\n",
    "## visual genome\n",
    "http://visualgenome.org/  \n",
    "様々なタスクに使えるデータがまとまったオールインワンセット  \n",
    "画像中の物体の位置、その物体の属性、物体間の関係性、画像の説明文、画像に対する質問応答のデータが一つにまとまっている。  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 評価方法のお話\n",
    "実は、「良い説明文を生成できているか」を定量的に評価することはとっても難しい。  \n",
    "ここでも機械翻訳の知見を使って評価されることが多い。  \n",
    "\n",
    "![image.png](https://www.fastpic.jp/images.php?file=1820247011.png)\n",
    "<div style=\"text-align: center;\">\n",
    "Image Captioningの評価指標の概要 \n",
    "引用：ぼくの修士1年の研究の発表資料\n",
    "</div>\n",
    "\n",
    "## BLEU\n",
    "機械翻訳の分野で最も広く使われる評価指標  \n",
    "正解文と予測文の間にN-gramの表現がどの程度出現したかの割合\n",
    "precisionに相当するので、分母は予測文の語数になる\n",
    "\n",
    "## ROUGE\n",
    "機械翻訳や文書要約の評価に使われる評価指標\n",
    "blueはprecisionを考慮した指標だが、こちらはrecallを考慮した指標\n",
    "\n",
    "## METEOR\n",
    "機械翻訳の分野で使われる評価指標\n",
    "事前に用意した類義語辞書を用いて、類義語も考慮したBLEUの計算を行う\n",
    "\n",
    "## CIDEr\n",
    "Image Captioningのために提案された評価指標\n",
    "F値(precision, recallの両方を考慮)を計算する指標で、tf-idfの考えを用いて重み付けを行う\n",
    "\n",
    "## SPICE\n",
    "こちらもImage Captioningのために提案された評価指標\n",
    "説明文をScene Graphと呼ばれるデータ構造に変換してから、Scene Graph中に出現するノード組間のF値を計算する\n",
    "\n",
    "![SPICEの例](http://www.panderson.me/images/spice-concept.png)\n",
    "<div style=\"text-align: center;\">\n",
    "SPICEの概要図\n",
    "引用：[SPICE: Semantic Propositional Image Caption Evaluation](http://www.panderson.me/spice/)\n",
    "</div>\n",
    "\n",
    "## HOIL it!\n",
    "正解のキャプションの一部を誤ったものに変えて、それを検知できるかで評価を行う"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Captioningの向かう先・近年の研究動向\n",
    "来た人限定！！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
